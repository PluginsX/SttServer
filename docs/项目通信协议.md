# FastAPI + FunASR 本地语音识别服务通信协议文档

## 1. 协议概述

本服务基于 FastAPI 框架与 FunASR 的 paraformer-zh-streaming 模型，提供本地部署的实时语音识别服务。该服务完全兼容阿里云DashScope WebSocket API规范，支持中文普通话的实时流式语音识别，使得原本调用阿里云Paraformer实时语音识别服务的客户端代码无需修改即可无缝接入本地服务。实时多模态交互协议（WebSocket）。

### 1.1 协议特点

- 100%兼容阿里云DashScope WebSocket API
- 支持实时流式语音识别
- 支持中文普通话识别
- 支持中间结果和最终结果返回
- 本地部署无需Token鉴权

### 1.2 适用场景

- 本地语音识别服务部署
- 降低阿里云API调用成本
- 数据隐私保护场景
- 离线语音识别需求

## 2. 连接建立与鉴权

### 2.1 WebSocket 连接 URL

- **格式**: `ws://<host>:<port>/api-ws/v1/inference`
- **示例**: `ws://localhost:8000/api-ws/v1/inference`
- **说明**: 本地部署无需Token鉴权，直接建立连接即可

### 2.2 握手流程

1. 客户端发起 WebSocket 连接请求
2. 服务端返回 101 Switching Protocols 响应，建立连接
3. 连接建立后，客户端发送 run-task 指令开始识别

### 2.3 Headers

| 参数 | 类型 | 是否必选 | 说明 |
|------|------|----------|------|
| Authorization | string | 否 | 本地部署无需鉴权，可省略 |
| user-agent | string | 否 | 客户端标识，便于服务端追踪来源 |

## 3. 通信流程时序

### 3.1 正常识别流程

```
客户端                    服务端
  │                          │
  │──── WebSocket连接 ──────→│
  │←──── 101响应 ────────────│
  │                          │
  │──── run-task指令 ───────→│
  │                          │
  │←──── task-started事件 ───│
  │                          │
  │──── 音频数据(二进制) ────→│
  │←──── result-generated事件│ (中间结果)
  │                          │
  │──── 音频数据(二进制) ────→│
  │←──── result-generated事件│ (中间结果)
  │                          │
  │──── 音频数据(二进制) ────→│
  │←──── result-generated事件│ (最终结果)
  │                          │
  │──── finish-task指令 ────→│
  │←──── result-generated事件│ (最终结果，如有)
  │←──── task-finished事件 ──│
  │                          │
  │──── 关闭连接 ───────────→│
```

### 3.2 异常处理流程

- **连接超时**: 服务端检测到长时间无数据流，主动关闭连接
- **音频格式错误**: 服务端返回错误信息，关闭连接
- **参数错误**: 服务端返回错误信息，关闭连接

## 4. 数据结构定义

### 4.1 通用 Header 结构

所有指令和事件都包含统一的 Header 结构：

#### 指令 Header（客户端 → 服务端）

```json
{
  "header": {
    "action": "string",      // 指令类型：run-task 或 finish-task
    "task_id": "string",     // 任务ID，32位UUID，整个会话中保持一致
    "streaming": "string"    // 固定值："duplex"
  }
}
```

#### 事件 Header（服务端 → 客户端）

```json
{
  "header": {
    "task_id": "string",     // 任务ID，与客户端发送的task_id一致
    "event": "string",       // 事件类型：task-started、result-generated、task-finished
    "attributes": {}         // 扩展属性，当前为空对象
  }
}
```

### 4.2 客户端指令

#### 4.2.1 run-task 指令

**功能**: 开始语音识别任务

**发送时机**: WebSocket连接建立后

**Payload**:
```json
{
  "header": {
    "action": "run-task",
    "task_id": "2bf83b9a-baeb-4fda-8d9a-xxxxxxxxxxxx",
    "streaming": "duplex"
  },
  "payload": {
    "task_group": "audio",
    "task": "asr",
    "function": "recognition",
    "model": "paraformer-realtime-v2",
    "parameters": {
      "format": "pcm",
      "sample_rate": 16000,
      "language_hints": ["zh"],
      "punctuation_prediction_enabled": true,
      "inverse_text_normalization_enabled": true
    },
    "input": {}
  }
}
```

**参数说明**:

| 参数 | 类型 | 是否必选 | 说明 |
|------|------|----------|------|
| header.action | string | 是 | 指令类型，固定为"run-task" |
| header.task_id | string | 是 | 任务ID，32位UUID，整个会话中保持一致 |
| header.streaming | string | 是 | 固定字符串："duplex" |
| payload.task_group | string | 是 | 固定字符串："audio" |
| payload.task | string | 是 | 固定字符串："asr" |
| payload.function | string | 是 | 固定字符串："recognition" |
| payload.model | string | 是 | 模型名称，固定为"paraformer-realtime-v2" |
| payload.parameters.format | string | 是 | 音频格式：pcm、wav、mp3、opus、speex、aac、amr |
| payload.parameters.sample_rate | integer | 是 | 采样率（Hz），支持任意值，推荐16000 |
| payload.parameters.language_hints | array | 否 | 语言提示：["zh"]表示中文普通话 |
| payload.parameters.punctuation_prediction_enabled | boolean | 否 | 是否添加标点，默认true |
| payload.parameters.inverse_text_normalization_enabled | boolean | 否 | 是否数字转写（ITN），默认true |
| payload.input | object | 是 | 固定格式：{} |

#### 4.2.2 finish-task 指令

**功能**: 结束语音识别任务

**发送时机**: 音频发送完成后

**Payload**:
```json
{
  "header": {
    "action": "finish-task",
    "task_id": "2bf83b9a-baeb-4fda-8d9a-xxxxxxxxxxxx",
    "streaming": "duplex"
  },
  "payload": {
    "input": {}
  }
}
```

**参数说明**:

| 参数 | 类型 | 是否必选 | 说明 |
|------|------|----------|------|
| header.action | string | 是 | 指令类型，固定为"finish-task" |
| header.task_id | string | 是 | 任务ID，需与run-task指令中的task_id保持一致 |
| header.streaming | string | 是 | 固定字符串："duplex" |
| payload.input | object | 是 | 固定格式：{} |

### 4.3 服务端事件

#### 4.3.1 task-started 事件

**触发时机**: 收到 run-task 指令后

**Payload**:
```json
{
  "header": {
    "task_id": "2bf83b9a-baeb-4fda-8d9a-xxxxxxxxxxxx",
    "event": "task-started",
    "attributes": {}
  },
  "payload": {}
}
```

**参数说明**:

| 参数 | 类型 | 说明 |
|------|------|------|
| header.event | string | 事件类型，固定为"task-started" |
| header.task_id | string | 任务ID，与客户端发送的task_id一致 |

#### 4.3.2 result-generated 事件

**触发时机**: 持续返回语音识别结果（包括中间结果和最终结果）

**中间结果示例**:
```json
{
  "header": {
    "task_id": "2bf83b9a-baeb-4fda-8d9a-xxxxxxxxxxxx",
    "event": "result-generated",
    "attributes": {}
  },
  "payload": {
    "output": {
      "sentence": {
        "begin_time": 170,
        "end_time": null,
        "text": "你好世界",
        "sentence_end": false
      }
    }
  }
}
```

**最终结果示例**:
```json
{
  "header": {
    "task_id": "2bf83b9a-baeb-4fda-8d9a-xxxxxxxxxxxx",
    "event": "result-generated",
    "attributes": {}
  },
  "payload": {
    "output": {
      "sentence": {
        "begin_time": 170,
        "end_time": 920,
        "text": "你好世界。",
        "sentence_end": true,
        "words": [
          {
            "begin_time": 170,
            "end_time": 295,
            "text": "你好",
            "punctuation": ""
          },
          {
            "begin_time": 295,
            "end_time": 503,
            "text": "世界",
            "punctuation": "。"
          }
        ]
      }
    },
    "usage": {
      "duration": 1
    }
  }
}
```

**参数说明**:

| 参数 | 类型 | 说明 |
|------|------|------|
| header.event | string | 事件类型，固定为"result-generated" |
| header.task_id | string | 任务ID，与客户端发送的task_id一致 |
| payload.output.sentence.begin_time | integer | 句子开始时间，单位ms |
| payload.output.sentence.end_time | integer\|null | 句子结束时间，null表示中间结果，数值表示最终结果 |
| payload.output.sentence.text | string | 识别文本 |
| payload.output.sentence.sentence_end | boolean | 是否句子结束，false表示中间结果，true表示最终结果 |
| payload.output.sentence.words | array | 字级别时间戳信息（可选） |
| payload.usage.duration | integer | 任务计费时长（秒），仅在sentence_end为true时返回 |

**判断中间/最终结果**:
- `end_time` 为 `null` 且 `sentence_end` 为 `false`: 中间结果
- `end_time` 有值且 `sentence_end` 为 `true`: 最终结果

#### 4.3.3 task-finished 事件

**触发时机**: 收到 finish-task 指令后

**Payload**:
```json
{
  "header": {
    "task_id": "2bf83b9a-baeb-4fda-8d9a-xxxxxxxxxxxx",
    "event": "task-finished",
    "attributes": {}
  },
  "payload": {}
}
```

**参数说明**:

| 参数 | 类型 | 说明 |
|------|------|------|
| header.event | string | 事件类型，固定为"task-finished" |
| header.task_id | string | 任务ID，与客户端发送的task_id一致 |

## 5. 音频数据格式

### 5.1 数据帧格式

- **类型**: WebSocket Binary Frame
- **编码**: PCM 16bit 小端序
- **声道**: 单声道 (Mono)
- **采样率**: 8000Hz 或 16000Hz（推荐）
- **分片大小**: 建议 3200 字节 (200ms @ 16kHz)

### 5.2 数据流要求

- 音频数据必须在收到 task-started 事件后发送
- 音频数据必须连续发送，不能有间隔
- 发送频率应与采样率匹配，避免数据堆积或丢失
- 建议每次发送100ms的音频，并间隔100ms

### 5.3 音频格式支持

| 格式 | 说明 |
|------|------|
| pcm | 原始PCM数据 |
| wav | WAV格式，必须为PCM编码 |
| mp3 | MP3格式 |
| opus | Opus格式，必须使用Ogg封装 |
| speex | Speex格式，必须使用Ogg封装 |
| aac | AAC格式 |
| amr | AMR格式，仅支持AMR-NB类型 |

## 6. 错误处理

### 6.1 错误类型

| 错误类型 | 说明 | 处理建议 |
|----------|------|----------|
| 连接错误 | WebSocket连接失败 | 检查服务端是否启动，检查网络连接 |
| 参数错误 | 指令参数格式错误 | 检查指令格式是否符合协议规范 |
| 音频格式错误 | 音频格式不支持 | 检查format和sample_rate参数 |
| 推理错误 | 模型推理失败 | 检查音频数据是否有效，检查模型是否正常加载 |
| 超时错误 | 连接超时 | 检查网络连接，检查服务端状态 |

### 6.2 错误处理流程

1. 服务端检测到错误
2. 服务端关闭WebSocket连接
3. 客户端收到连接关闭事件
4. 客户端根据错误类型决定是否重试

### 6.3 错误码映射

| 错误码 | 说明 | 处理建议 |
|--------|------|----------|
| 1000 | 正常关闭 | 正常流程 |
| 1001 | 端点离开 | 客户端或服务端主动关闭 |
| 1002 | 协议错误 | 检查协议实现 |
| 1003 | 不支持的数据类型 | 检查数据格式 |
| 1006 | 连接异常关闭 | 检查网络连接 |
| 1011 | 内部错误 | 检查服务端日志 |

## 7. 性能指标

### 7.1 性能目标

| 指标 | 目标值 | 说明 |
|------|--------|------|
| 首字延迟 | < 600ms | 从开始接收音频到返回第一个识别结果的时间 |
| 识别准确率 | > 95% | 中文普通话场景 |
| 并发支持 | 5-10路 | 取决于GPU显存 |
| 内存占用 | < 4GB | 单路识别 |
| CPU使用率 | < 80% | 正常运行时 |

### 7.2 性能优化建议

- 使用GPU加速推理
- 优化音频分块大小（建议200ms@16kHz）
- 实现模型预热机制
- 优化缓冲区管理策略

## 8. 安全考虑

### 8.1 本地部署安全

- 本地部署无需Token鉴权，建议在内网环境使用
- 如需外网访问，建议添加反向代理和访问控制
- 建议使用HTTPS（wss）加密传输

### 8.2 数据隐私

- 音频数据仅在本地处理，不会上传到云端
- 识别结果仅在本地返回，不会保存到服务端
- 建议定期清理临时音频数据

## 9. 兼容性说明

### 9.1 与阿里云DashScope API的兼容性

本服务完全兼容阿里云DashScope WebSocket API，客户端代码仅需修改WebSocket URL即可：

| 阿里云 | 本地服务 |
|--------|----------|
| `wss://dashscope.aliyuncs.com/api-ws/v1/inference` | `ws://localhost:8000/api-ws/v1/inference` |
| 需要Authorization Header | 无需Authorization Header |
| 支持多语言模型 | 仅支持中文普通话 |
| 需要API Key | 无需API Key |

### 9.2 功能差异

| 功能 | 阿里云DashScope | 本地服务 |
|------|-----------------|----------|
| 实时流式识别 | ✅ | ✅ |
| 中间结果返回 | ✅ | ✅ |
| 标点符号预测 | ✅ | ✅ |
| 数字转写（ITN） | ✅ | ✅ |
| 多语言支持 | ✅ | ❌ 仅中文普通话 |
| 情感识别 | ✅ | ❌ |
| 热词定制 | ✅ | ❌ |
| 语义断句 | ✅ | ❌ |

## 10. 参考文档

- [Paraformer实时语音识别WebSocket API.md](Paraformer实时语音识别WebSocket API.md) - 阿里云官方API文档
- [项目开发计划.md](项目开发计划.md) - 项目开发计划
- FunASR官方文档: https://github.com/alibaba-damo-academy/FunASR
- FastAPI官方文档: https://fastapi.tiangolo.com/

## 11. 附录

### 11.1 术语表

| 术语 | 说明 |
|------|------|
| ASR | Automatic Speech Recognition，自动语音识别 |
| PCM | Pulse Code Modulation，脉冲编码调制 |
| WebSocket | 一种全双工通信协议 |
| DashScope | 阿里云大模型服务平台 |
| FunASR | 阿里达摩院开源的语音识别工具包 |
| Paraformer | 阿里达摩院研发的语音识别模型 |
| ITN | Inverse Text Normalization，逆文本正则化 |

### 11.2 常见问题

**Q1: 本地服务与阿里云API的主要区别是什么？**
A: 本地服务无需API调用费用，数据隐私保护更好，但功能相对简化（仅支持中文普通话）。

**Q2: 如何生成task_id？**
A: 使用32位UUID，大多数编程语言都内置了生成UUID的API。例如Python：
```python
import uuid
task_id = uuid.uuid4().hex
```

**Q3: 中间结果和最终结果有什么区别？**
A: 中间结果的`end_time`为`null`，`sentence_end`为`false`，表示识别还在进行中；最终结果的`end_time`有值，`sentence_end`为`true`，表示句子识别完成。

**Q4: 音频分片大小如何选择？**
A: 建议使用200ms的音频（16kHz下为3200字节），可以在延迟和准确率之间取得平衡。

**Q5: 如何处理连接断开？**
A: 客户端应监听WebSocket的close事件，根据关闭代码判断是否需要重试。

### 11.3 版本历史

| 版本 | 日期 | 说明 |
|------|------|------|
| v1.0 | 2026-02-11 | 初始版本，完全兼容阿里云DashScope WebSocket API |

---

**文档维护**: 本文档应随着项目进展持续更新，确保与实际实现保持一致。
